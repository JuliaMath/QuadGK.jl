<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Quadrature rules · QuadGK.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">QuadGK.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../quadgk-examples/">Examples</a></li><li class="is-active"><a class="tocitem" href>Quadrature rules</a><ul class="internal"><li><a class="tocitem" href="#Quadrature-rules-and-Gaussian-quadrature"><span>Quadrature rules and Gaussian quadrature</span></a></li></ul></li><li><a class="tocitem" href="../weighted-gauss/">Weighted quadrature</a></li><li><a class="tocitem" href="../api/">API reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Quadrature rules</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Quadrature rules</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaMath/QuadGK.jl/blob/master/docs/src/gauss-kronrod.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Gauss-and-Gauss–Kronrod-quadrature-rules"><a class="docs-heading-anchor" href="#Gauss-and-Gauss–Kronrod-quadrature-rules">Gauss and Gauss–Kronrod quadrature rules</a><a id="Gauss-and-Gauss–Kronrod-quadrature-rules-1"></a><a class="docs-heading-anchor-permalink" href="#Gauss-and-Gauss–Kronrod-quadrature-rules" title="Permalink"></a></h1><p>The foundational algorithm of the QuadGK package is a <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Kronrod_quadrature_formula">Gauss–Kronrod quadrature rule</a>, an extension of <a href="https://en.wikipedia.org/wiki/Gaussian_quadrature">Gaussian quadrature</a>. In this chapter of the QuadGK manual, we briefly explain what these are, and describe how you can use QuadGK to generate your own Gauss and Gauss–Kronrod rules, including for more complicated weighted integrals.</p><h2 id="Quadrature-rules-and-Gaussian-quadrature"><a class="docs-heading-anchor" href="#Quadrature-rules-and-Gaussian-quadrature">Quadrature rules and Gaussian quadrature</a><a id="Quadrature-rules-and-Gaussian-quadrature-1"></a><a class="docs-heading-anchor-permalink" href="#Quadrature-rules-and-Gaussian-quadrature" title="Permalink"></a></h2><p>A <strong>quadrature rule</strong> is simply a way to approximate an integral by a sum:</p><p class="math-container">\[\int_a^b f(x) dx \approx \sum_{i=1}^n w_i f(x_i)\]</p><p>where the <span>$n$</span> evaluation points <span>$x_i$</span> are known as the <strong>quadrature points</strong> and the coefficients <span>$w_i$</span> are the <strong>quadrature weights</strong>.   We typically want to design quadrature rules that are as accurate as possible for as small an <code>n</code> as possible, for a wide range of functions <span>$f(x)$</span> (for example, for <a href="https://en.wikipedia.org/wiki/Smoothness">smooth functions</a>). The underlying assumption is that evaluating the integrand <span>$f(x)$</span> is computationally expensive, so you want to do this as few times as possible for a given error tolerance.</p><p>There are <a href="https://en.wikipedia.org/wiki/Numerical_integration">many numerical-integration techniques</a> for designing quadrature rules.  For example, one could simply pick the points <span>$x_i$</span> uniformly at random in <span>$(a,b)$</span> and use a weight <span>$w_i = 1/n$</span> to take the average — this is <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte-Carlo integration</a>, which is simple but converges rather slowly (its error scales as <span>$\sim 1/\sqrt{n}$</span>).</p><p>A particularly efficient class of quadrature rules is known as <a href="https://en.wikipedia.org/wiki/Gaussian_quadrature">Gaussian quadrature</a>, which exploits the remarkable theory of <a href="https://en.wikipedia.org/wiki/Orthogonal_polynomials">orthogonal polynomials</a> in order to design <span>$n$</span>-point rules that <em>exactly</em> integrate all polynomial functions <span>$f(x)$</span> up to degree <span>$2n-1$</span>.  More importantly, the error goes to zero extremely rapidly even for non-polynomial <span>$f(x)$</span>, as long as <span>$f(x)$</span> is sufficiently smooth. (They converge <em>exponentially</em> rapidly for <a href="https://en.wikipedia.org/wiki/Analytic_function">analytic functions</a>.)  There are many variants of Gaussian quadrature, as we will discuss further below, but the specific case of computing <span>$\int_{-1}^{1} f(x) dx$</span> is known as <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Legendre_quadrature">Gauss–Legendre quadrature</a>, and <span>$\int_a^b f(x) dx$</span> over other intervals <span>$(a,b)$</span> is equivalent to Gauss–Legendre under a simple change of variables (given explicitly below).</p><p>The QuadGK package can compute the points <span>$x_i$</span> and weights <span>$w_i$</span> of a Gauss–Legendre quadrature rule (optionally rescaled to an arbitrary interval <span>$(a,b)$</span>) for you via the <a href="../api/#QuadGK.gauss-Tuple{Any, Integer, Real, Real}"><code>gauss</code></a> function. For example, the <span>$n=5$</span> point rule for integrating from <span>$a=1$</span> to <span>$b=3$</span> is computed by:</p><pre><code class="language-none">julia&gt; a = 1; b = 3; n = 5;

julia&gt; x, w = gauss(n, a, b);

julia&gt; [x w] # show points and weights as a 2-column matrix
5×2 Matrix{Float64}:
 1.09382  0.236927
 1.46153  0.478629
 2.0      0.568889
 2.53847  0.478629
 2.90618  0.236927
 ```
We can see that there are 5 points $a &lt; x_i &lt; b$.  They are *not* equally spaced or equally weighted, nor do they quite reach the endpoints.  We can now approximate integrals by evaluating the integrand $f(x)$ at these points, multiplying by the weights, and summing.  For example, $f(x)=\cos(x)$ can be integrated via:</code></pre><p>julia&gt; sum(w .* cos.(x)) # evaluate ∑ᵢ wᵢ f(xᵢ) -0.7003509770773674</p><p>julia&gt; sin(3) - sin(1)   # the exact integral -0.7003509767480293</p><pre><code class="language-none">Even with just $n = 5$ points, Gaussian quadrature can integrate such
a smooth function as this to 8–9 significant digits!

The `gauss` function allows you to compute Gaussian quadrature
rules to any desired precision, even supporting [arbitrary-precision arithmetic](https://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic) types such as `BigFloat`.  For example, we can compute the same rule as above to about 30 digits:</code></pre><p>julia&gt; setprecision(30, base=10);</p><p>julia&gt; x, w = gauss(BigFloat, n, a, b); @show x; @show w; x = BigFloat[1.0938201540613360072023731217019, 1.4615306898943169089636855793001, 2.0, 2.5384693101056830910363144207015, 2.9061798459386639927976268782981] w = BigFloat[0.23692688505618908751426404072106, 0.47862867049936646804129151483584, 0.56888888888888888888888888888975, 0.47862867049936646804129151483584, 0.23692688505618908751426404072106]</p><pre><code class="language-none">This allows you to compute numerical integrals to very high accuracy if you want.  (The [`quadgk`](@ref) function also supports arbitrary-precision arithmetic types.)

## Gauss–Kronrod: Error estimation and embedded rules

A good quadrature rule is often not enough: you also want
to have an **estimate of the error** for a given $f(x)$, in order to
decide whether you are happy with your approximate integral or if you
want to get a more accurate estimate by increasing $n$.

The most basic way to do this is to evaluate *two* quadrature rules, one with
fewer points $n&#39; &lt; n$, and use their *difference* as an error
estimate.  (If the error is rapidly converging with $n$, this is usually
a conservative upper bound on the error.)</code></pre><p>math \mbox{error estimate} \lesssim \left| \underbrace{\sum<em>{i=1}^n w</em>i f(x<em>i)}</em>\mbox{first rule} - \underbrace{\sum<em>{j=1}^{n&#39;} w</em>j&#39; f(x<em>j&#39;)}</em>\mbox{second rule} \right|</p><pre><code class="language-none">Naively, this requirs us to evaluate our integrand $f(x)$ an extra
$n&#39;$ times to get the error estimate from the second rule.  However,
we can do better: if the points $\{ x_j&#39; \}$ of the second ($n&#39;$-point) rule
are a *subset* of the points $\{ x_i \}$ of the points from the first
($n$-point) rule, then we only need $n$ function evaluations for the
first rule and can *re-use* them when evaluating the second rule.
This is called an **embedded** (or **nested**) quadrature rule.

There are many ways of designing embedded quadrature rules.  Unfortunately,
the nice Gaussian quadrature rules cannot be directly nested: the $n&#39;$-point
Gaussian quadrature points are *not* a subset of the $n$-point Gaussian
quadrature points for *any* $n&#39; &lt; n$.   Fortunately, there is a slightly
modified scheme that works, called [Gauss–Kronrod quadrature](https://en.wikipedia.org/wiki/Gauss%E2%80%93Kronrod_quadrature_formula): if you start with an $n&#39;-point$ Gaussian-quadrature scheme, you can extend it with
$n&#39;+1$ additional points to obtain a quadrature scheme with $n=2n&#39;+1$
points that exactly integrates polynomials up to degree $3N&#39;+1$.
Although this is slightly worse than an $n$-point Gaussian quadrature
scheme, it is still quite accurate, still converges very fast
for smooth functions, and gives you a built-in error estimate that
requires no additional function evaluations.   (In QuadGK, we refer
to the size $n&#39;$ of the embedded Gauss rule as the &quot;order&quot;, although
other authors use that term to refer to the degree of polynomials
that are integrated exactly.)

The [`quadgk`](@ref) function uses Gauss–Kronrod quadrature internally,
defaulting to order $n&#39;=7$ (i.e. $n=15$ points), though you can change
this with the `order` parameter.   This gives it both an estimated
integral and an estimated error.  If the error is larger than your requested
tolerance, `quadgk` splits the integration interval into two halves and
applies the same Gauss–Kronrod rule to each half, and continues to
subdivide the intervals until the desired tolerance is achieved, a
process called $h$-[adaptive quadrature](https://en.wikipedia.org/wiki/Adaptive_quadrature).  (An alternative called $p$-adaptive quadrature
would increase the order $n&#39;$ on the same interval.  $h$-adaptive
quadrature is more robust if your function has localized bad behaviors
like sharp peaks or discontinuities, because it will progressively
add more points mostly in these &quot;bad&quot; regions.)

You can use the [`kronrod`](@ref) function to compute a Gauss–Kronrod
rule to any desired order (and to any precision).  For example, we can extend our 5-point Gaussian-quadrature rule for $\int_1^3$ from the previous section to an 11-point ($2n+1$) Gauss-Kronrod rule:</code></pre><p>julia&gt; x, w, gw = kronrod(n, a, b); [ x w ] # points and weights 11×2 Matrix{Float64}:  1.01591  0.042582  1.09382  0.115233  1.24583  0.186801  1.46153  0.24104  1.72037  0.27285  2.0      0.282987  2.27963  0.27285  2.53847  0.24104  2.75417  0.186801  2.90618  0.115233  2.98409  0.042582</p><pre><code class="language-none">Similar to Gaussian quadrature, notice that all of the Gauss–Kronrod points
$a &lt; x_i &lt; b$ lie in the interior $(a,b)$ of our integration interval,
and that they are unequally spaced (clustered more near the edges).
The third return value, `gw`, gives the weights of the embedded 5-point
Gaussian-quadrature rule, which corresponds to the *even-indexed* points
`x[2:2:end]` of the 11-point Gauss–Kronrod rule:</code></pre><p>julia&gt; [ x[2:2:end] gw ] # embedded Gauss points and weights 5×2 Matrix{Float64}:  1.09382  0.236927  1.46153  0.478629  2.0      0.568889  2.53847  0.478629  2.90618  0.236927</p><pre><code class="language-none">So, we can evaluate our integrand $f(x)$ at the 11 Gauss–Kronrod points, and then re-use 5 of these values to obtain an error estimate.  For example, with $f(x) = \cos(x)$, we obtain:</code></pre><p>julia&gt; fx = cos.(x); # evaluate f(xᵢ)</p><p>julia&gt; integral = sum(w .* fx) # ∑ᵢ wᵢ f(xᵢ) -0.7003509767480292</p><p>julia&gt; error = abs(integral - sum(gw .* fx[2:2:end])) # |integral - ∑ⱼ wⱼ′ f(xⱼ′)| 3.2933822335934337e-10</p><p>julia&gt; abs(integral - (sin(3) - sin(1))) # true error ≈ machine precision 1.1102230246251565e-16</p><pre><code class="language-none">As noted above, the error estimate tends to actually be quite a conservative
upper bound on the error, because it is effectively a measure of the error of the lower-order *embedded* 5-point Gauss rule rather than that of the higher-order 11-point Gauss–Kronrod rule.  For smooth functions like $\cos(x)$, an 11-point rule can have an error orders of magnitude smaller than that of the 5-point rule.  (Here, the 11-point rule&#39;s accuracy
is so good that it is actually limited by [floating-point roundoff error](https://en.wikipedia.org/wiki/Machine_epsilon); in infinite precision the error would have been `≈ 6e-23`.)

You may notice that both the Gauss–Kronrod and the Gaussian quadrature
rules are *symmetric* around the center $(a+b)/2$ of the integration interval.   In fact, we provide a lower-level function `kronrod(n)` that only computes roughly the first half of the points and weights for $\int_{-1}^{1}$ ($b = -a = 1$), corresponding to $x_i \le 0$.</code></pre><p>julia&gt; x, w, gw = kronrod(5); [x w] # points xᵢ ≤ 0 and weights 6×2 Matrix{Float64}:  -0.984085  0.042582  -0.90618   0.115233  -0.754167  0.186801  -0.538469  0.24104  -0.27963   0.27285   0.0       0.282987</p><p>julia&gt; [x[2:2:end] gw] # embedded Gauss points ≤ 0 and weights 3×2 Matrix{Float64}:  -0.90618   0.236927  -0.538469  0.478629   0.0       0.568889</p><pre><code class="language-none">Of course, you still have to evaluate $f(x)$ at all $2n+1$ points,
but summing the results requires a bit less arithmetic
and storing the rule takes less memory.  Note also that the $(-1,1)$ rule can be applied to any desired interval $(a,b)$ by a change of variables</code></pre><p>math \int<em>a^b f(x) dx = \frac{b-a}{2} \int</em>{-1}^{+1} f\left( (u+1)\frac{b-a}{2} + a  \right) \, ,</p><pre><code class="language-none">so the $(-1,1)$ rule can be computed once (for a given order and precision) and re-used.  In consequence, `kronrod(n)` is `quadgk` uses internally.  The higher-level `kronrod(n, a, b)` function is more convenient for casual use, however.

As with `gauss`, the `kronrod` function works with arbitrary precision,
such as `BigFloat` numbers.  `kronrod(n, a, b)` uses the precision of
the endpoints `(a,b)` (converted to floating point), while for
`kronrod(n)` you can explicitly pass a floating-point type `T` as
the first argument, e.g. for 50-digit precision:</code></pre><p>julia&gt; setprecision(50, base=10); x, w, gw = kronrod(BigFloat, 5); x 6-element Vector{BigFloat}:  -0.9840853600948424644961729346361394995805528241884714  -0.9061798459386639927976268782993929651256519107625304  -0.7541667265708492204408171669461158663862998043714845  -0.5384693101056830910363144207002088049672866069055604  -0.2796304131617831934134665227489774362421188153561727   0.0</p><pre><code class="language-none">
## Quadrature rules for weighted integrals

More generally, one can compute quadrature rules for a **weighted** integral:
</code></pre><p>math \int<em>a^b w(x) f(x) dx \approx \sum</em>{i=1}^n w<em>i f(x</em>i) `<span>$where the effect of **weight function** $w(x)$ (usually required to be $≥ 0$ in$</span>(a,b)``) is included in the quadrature weights <span>$w_i$</span> and points <span>$x_i$</span>.   The main motivation for weighted quadrature rules is to handle <em>poorly behaved</em> integrands — singular, discontinuous, highly oscillatory, and so on — where the &quot;bad&quot; behavior is <em>known</em> and can be <em>factored out</em> into <span>$w(x)$</span>.  By designing a quadrature rule with <span>$w(x)$</span> taken into account, one can obtain fast convergence as long as the remaining factor <span>$f(x)$</span> is smooth, regardless of how &quot;bad&quot; <span>$w(x)$</span> is.  Moreover, the rule can be re-used for many different <span>$f(x)$</span> as long as <span>$w(x)$</span> remains the same.</p><p>The QuadGK package can compute both Gauss and Gauss–Kronrod quadrature rules for arbitrary weight functions <span>$w(x)$</span>, to arbitrary precision, as described in the section: <a href="../weighted-gauss/#Gaussian-quadrature-and-arbitrary-weight-functions">Gaussian quadrature and arbitrary weight functions</a>.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../quadgk-examples/">« Examples</a><a class="docs-footer-nextpage" href="../weighted-gauss/">Weighted quadrature »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 25 July 2023 04:47">Tuesday 25 July 2023</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
